{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'annotations'\n",
    "dataset_name = 'nwpu'\n",
    "data_folder = 'data'\n",
    "google_drive_folder = 'google_drive'\n",
    "merged_file = 'tiles_karl2.json'\n",
    "s2cloudless_annotation_file_path = os.path.join(base_folder, )\n",
    "flag_write_files = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read a `json` file that contains a dictionary of image names and its annotations. Then, it will iterate over each pair and create a new `json` file with the same name as the image and respectively annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 files have been extracted with success!\n"
     ]
    }
   ],
   "source": [
    "if flag_write_files:\n",
    "    s2cloudless_annotation_file_path = os.path.join(base_folder, merged_file)\n",
    "    flag_file_exists = os.path.exists(s2cloudless_annotation_file_path)\n",
    "    if flag_file_exists:\n",
    "        annotations = json.load(open(s2cloudless_annotation_file_path))\n",
    "        for image_name, annotation in annotations.items():\n",
    "            image_name_without_extension = image_name.split('.')[0]\n",
    "            annotation_file_name = f'{image_name_without_extension}.json'\n",
    "            annotation_file_path = os.path.join(base_folder, dataset_name, annotation_file_name)\n",
    "            with open(annotation_file_path, 'w') as outfile:\n",
    "                annotation_pair = {image_name: annotation}\n",
    "                json.dump(annotation_pair, outfile)\n",
    "        print(f'{len(annotations)} files have been extracted with success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if there are missing labels in the NWPU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annotations= [f'lake_{i:03d}.json' for i in range(1, 701)]\n",
    "expected_annotations = set(all_annotations)\n",
    "assert len(expected_annotations) == 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_query = os.path.join(base_folder, dataset_name, '*.json')\n",
    "annotations_file_paths = glob.glob(annotation_query)\n",
    "existing_annotations = [path.split(os.sep)[-1] for path in annotations_file_paths]\n",
    "annotations = set(existing_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['lake_351.json',\n",
      " 'lake_352.json',\n",
      " 'lake_354.json',\n",
      " 'lake_356.json',\n",
      " 'lake_358.json',\n",
      " 'lake_359.json',\n",
      " 'lake_360.json',\n",
      " 'lake_361.json',\n",
      " 'lake_362.json',\n",
      " 'lake_363.json',\n",
      " 'lake_364.json',\n",
      " 'lake_366.json',\n",
      " 'lake_367.json',\n",
      " 'lake_368.json',\n",
      " 'lake_369.json',\n",
      " 'lake_641.json',\n",
      " 'lake_645.json',\n",
      " 'lake_659.json']\n"
     ]
    }
   ],
   "source": [
    "missing_annotations = expected_annotations - annotations\n",
    "print(len(missing_annotations))\n",
    "pprint.pprint(sorted(missing_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy images that contain annotations to the good_drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "dataset_name = 'nwpu_images'\n",
    "annotation_query = os.path.join(base_folder, 'nwpu', '*.json')\n",
    "annotations_file_paths = glob.glob(annotation_query)\n",
    "for file_path in annotations_file_paths:\n",
    "    annotations_file = file_path.split(os.sep)[-1].replace('json', 'jpg')\n",
    "    source_file = os.path.join(dataset_name, data_folder, annotations_file)\n",
    "    destination_file = os.path.join(google_drive_folder, dataset_name, data_folder, annotations_file)\n",
    "    #copyfile(source_file, destination_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "dataset_name = 's2cloudless_imagery'\n",
    "annotation_query = os.path.join(base_folder, 's2cloudless', '*.json')\n",
    "annotations_file_paths = glob.glob(annotation_query)\n",
    "for file_path in annotations_file_paths:\n",
    "    annotations_file = file_path.split(os.sep)[-1].replace('json', 'jpg')\n",
    "    source_file = os.path.join(dataset_name, data_folder, annotations_file)\n",
    "    destination_file = os.path.join(google_drive_folder, dataset_name, data_folder, annotations_file)\n",
    "    #copyfile(source_file, destination_file)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
